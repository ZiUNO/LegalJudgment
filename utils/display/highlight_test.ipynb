{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from bertviz import model_view\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "model_version = r\"E:/PyCharmWorkspace/LegalJudgment/utils/model/torch_pretrained_bert_multi_label/tmp/self\"\n",
    "do_lower_case = True\n",
    "model = BertForSequenceClassification.from_pretrained(model_version, num_labels=202, output_attentions=True)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_version, do_lower_case=do_lower_case)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "sentence_a = \"被告人周某在越野车内窃得黑色手机。\"\n",
    "# sentence_b = \"马某被孙某打伤\"\n",
    "sentence_b = None\n",
    "\n",
    "hide_delimiter_attn = False\n",
    "\n",
    "inputs = tokenizer.encode_plus(sentence_a, sentence_b, return_tensors='pt', add_special_tokens=True)\n",
    "token_type_ids = inputs['token_type_ids']\n",
    "input_ids = inputs['input_ids']\n",
    "\n",
    "logits = model(input_ids, token_type_ids=token_type_ids)\n",
    "attention = logits[-1]\n",
    "\n",
    "input_id_list = input_ids[0].tolist()  # Batch index 0\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_id_list)\n",
    "\n",
    "if sentence_b:\n",
    "    sentence_b_start = token_type_ids[0].tolist().index(1)\n",
    "else:\n",
    "    sentence_b_start = None\n",
    "if hide_delimiter_attn:\n",
    "    for i, t in enumerate(tokens):\n",
    "        if t in (\"[SEP]\", \"[CLS]\"):\n",
    "            for layer_attn in attention:\n",
    "                layer_attn[0, :, i, :] = 0\n",
    "                layer_attn[0, :, :, i] = 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# squeezed = []\n",
    "# for layer_attention in attention:\n",
    "#     # num_heads x seq_len x seq_len\n",
    "#     squeezed.append(layer_attention.squeeze(0))\n",
    "# import torch\n",
    "# attn = torch.stack(squeezed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# model_view(attention, tokens, sentence_b_start)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "attention_matrix = list(attention)\n",
    "attention_matrix = [att.detach().numpy() for att in attention_matrix]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# attention_matrix = np.sum(attention_matrix, axis=0)[0]\n",
    "# attention_matrix = np.sum(attention_matrix, axis=0)\n",
    "# \n",
    "# attention_matrix = attention_matrix / (12 * 12)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "result2save = logits[0].detach().numpy()[0]\n",
    "attention2save = attention_matrix"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "result_s = [str(r) for r in result2save]\n",
    "result_s = \"\\n\".join(result_s)\n",
    "\n",
    "with open(\"./result.txt\", 'w', newline='\\n') as f:\n",
    "    f.write(result_s)\n",
    "\n",
    "# attention_s = [\" \".join([str(tmp) for tmp in list(i)]) for i in attention2save]\n",
    "# attention_s = \"\\n\".join(attention_s)\n",
    "# with open(\"./attention.txt\", \"w\", newline='\\n') as f:\n",
    "#     f.write(attention_s)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "attention_json = {}\n",
    "for i, lay in enumerate(attention2save):\n",
    "    attention_json[i] = {}\n",
    "    for j, head in enumerate(lay[0]):\n",
    "        head2save = []\n",
    "        for vec in head:\n",
    "            head2save.append([str(tmp) for tmp in vec])\n",
    "        attention_json[i][j] = head2save\n",
    "import json\n",
    "with open(\"./attention.json\", \"w\") as f:\n",
    "    json.dump(attention_json, f)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}